{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hg0k1kHQ632O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "880a156c-401d-4393-a1e6-281de4f2e86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.0.0)\n",
            "Requirement already satisfied: torch in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
            "Requirement already satisfied: openpyxl in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.5)\n",
            "Requirement already satisfied: accelerate in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.34.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.15.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (72.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: et-xmlfile in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (5.9.8)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\michael\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Michael\\\\Downloads\\\\BERTmodel\\\\comments.xlsx'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load the Excel file from the local directory\u001b[39;00m\n\u001b[0;32m     13\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMichael\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBERTmodel\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcomments.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Update with your local path\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Use the correct column names based on the inspection\u001b[39;00m\n\u001b[0;32m     17\u001b[0m correct_comment_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcareprovidercomments\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Michael\\\\Downloads\\\\BERTmodel\\\\comments.xlsx'"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch pandas scikit-learn openpyxl accelerate -U\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import torch\n",
        "\n",
        "# Load the Excel file from the local directory\n",
        "file_path = r\"C:\\Users\\Michael\\Downloads\\BERTmodel\\comments.xlsx\"  # Update with your local path\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Use the correct column names based on the inspection\n",
        "correct_comment_column = 'careprovidercomments'\n",
        "sentiment_score_column = 'Combined_Sentiment'\n",
        "bert_sentiment_column = 'BERT_Sentiment'\n",
        "\n",
        "# Work with the first 1000 rows\n",
        "data = data.iloc[:1000]\n",
        "\n",
        "# Drop rows with NaN values in the sentiment score column\n",
        "data = data.dropna(subset=[sentiment_score_column])\n",
        "\n",
        "# Print unique values in the sentiment score column after dropping NaNs\n",
        "print(\"Unique values in the sentiment score column after dropping NaNs:\", data[sentiment_score_column].unique())\n",
        "\n",
        "# Scale sentiment scores: 1-2 as 1, 3 as 2, 4-5 as 3\n",
        "def scale_sentiment_score(score):\n",
        "    if score in [1, 2]:\n",
        "        return 1\n",
        "    elif score == 3:\n",
        "        return 2\n",
        "    elif score in [4, 5]:\n",
        "        return 3\n",
        "\n",
        "data[sentiment_score_column] = data[sentiment_score_column].apply(scale_sentiment_score)\n",
        "\n",
        "# Print unique values in the sentiment score column after scaling\n",
        "print(\"Unique values in the sentiment score column after scaling:\", data[sentiment_score_column].unique())\n",
        "\n",
        "# Function to clean punctuation\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove all punctuation except periods\n",
        "        text = re.sub(r'[^\\w\\s.]', '', text)\n",
        "    else:\n",
        "        text = ''\n",
        "    return text\n",
        "\n",
        "# Clean the comments\n",
        "data['cleaned_comment'] = data[correct_comment_column].apply(clean_text)\n",
        "\n",
        "# Ensure sentiment scores are between 1 and 3\n",
        "def ensure_sentiment_scores(data, column):\n",
        "    if not data[column].between(1, 3).all():\n",
        "        raise ValueError(\"Sentiment scores must be between 1 and 3.\")\n",
        "\n",
        "ensure_sentiment_scores(data, sentiment_score_column)\n",
        "\n",
        "# Convert sentiment scores to 0-based index\n",
        "data[sentiment_score_column] = data[sentiment_score_column] - 1\n",
        "\n",
        "# Split the data into training and testing sets (80/20 split)\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the sizes of the training and testing sets\n",
        "print(f\"Training set size: {len(train_data)} comments\")\n",
        "print(f\"Test set size: {len(test_data)} comments\")\n",
        "\n",
        "# Free up memory by deleting the original dataframe\n",
        "del data\n",
        "\n",
        "# Convert the data to Hugging Face datasets\n",
        "train_dataset = Dataset.from_pandas(train_data)\n",
        "test_dataset = Dataset.from_pandas(test_data)\n",
        "dataset_dict = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
        "\n",
        "# Free up memory by deleting the Pandas dataframes\n",
        "del train_data, test_data\n",
        "\n",
        "# Initialize tokenizer with smaller max_length\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['cleaned_comment'], padding='max_length', truncation=True, max_length=64)\n",
        "\n",
        "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
        "\n",
        "def format_labels(examples):\n",
        "    examples[\"labels\"] = [int(label) for label in examples[sentiment_score_column]]\n",
        "    return examples\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.map(format_labels, batched=True)\n",
        "\n",
        "# Free up memory by deleting the original datasets\n",
        "del dataset_dict\n",
        "\n",
        "# Print tokenized dataset sizes for debugging\n",
        "print(f\"Tokenized training set size: {len(tokenized_datasets['train'])} comments\")\n",
        "print(f\"Tokenized test set size: {len(tokenized_datasets['test'])} comments\")\n",
        "\n",
        "# Load the model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Training arguments with reduced batch sizes\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=r\"C:\\Users\\Michael\\Downloads\\BERT_MODEL_TRAINING_RESULTS-20240701T162037Z-002\",  # Update with your local path\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        ")\n",
        "\n",
        "# Define evaluation metrics\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
        "    acc = accuracy_score(p.label_ids, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Print expected number of steps per epoch\n",
        "expected_steps_per_epoch = len(tokenized_datasets['train']) // training_args.per_device_train_batch_size\n",
        "print(f\"Expected number of steps per epoch: {expected_steps_per_epoch}\")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Unified predict function\n",
        "def predict(texts, model, tokenizer, batch_size=4):\n",
        "    model.eval()\n",
        "    if not isinstance(texts, list):\n",
        "        texts = texts.tolist()\n",
        "\n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        encoded_inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_inputs)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        batch_probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        batch_predictions = torch.argmax(batch_probabilities, dim=-1)\n",
        "\n",
        "        predictions.extend(batch_predictions.cpu().numpy())\n",
        "        probabilities.extend(batch_probabilities.cpu().numpy())\n",
        "\n",
        "    return np.array(predictions), np.array(probabilities)\n",
        "\n",
        "def evaluate_model(test_dataset, model, tokenizer):\n",
        "    texts = test_dataset['cleaned_comment']\n",
        "    true_labels = test_dataset[sentiment_score_column].tolist()\n",
        "\n",
        "    predictions, probabilities = predict(texts, model, tokenizer)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    return predictions, probabilities\n",
        "\n",
        "# Convert the test dataset to a pandas DataFrame\n",
        "test_data = tokenized_datasets['test'].to_pandas()\n",
        "\n",
        "# Evaluate the model\n",
        "predictions, probabilities = evaluate_model(test_data, model, tokenizer)\n",
        "\n",
        "# Inspect the predictions and their probabilities\n",
        "\n",
        "for i in range(len(test_data)):\n",
        "    print(f\"Text: {test_data['cleaned_comment'].iloc[i]}\")\n",
        "    print(f\"True Label: {test_data[sentiment_score_column].iloc[i] + 1}\")\n",
        "    print(f\"Predicted Label: {predictions[i] + 1}\")\n",
        "    print(f\"Probabilities: {probabilities[i]}\")\n",
        "    print()\n",
        "\n",
        "# Re-load the full dataset including comments without sentiment scores\n",
        "full_data = pd.read_excel(file_path)\n",
        "\n",
        "# Clean the comments\n",
        "full_data['cleaned_comment'] = full_data[correct_comment_column].apply(clean_text)\n",
        "\n",
        "# Predict BERT sentiment scores for all comments\n",
        "comments_to_predict = full_data['cleaned_comment'].tolist()\n",
        "predicted_labels, predicted_probabilities = predict(comments_to_predict, model, tokenizer)\n",
        "\n",
        "# Add predicted labels to the data in the BERT_Sentiment column\n",
        "full_data[bert_sentiment_column] = predicted_labels + 1  # Convert back to 1-based index\n",
        "\n",
        "# List of columns to keep\n",
        "columns_to_keep =['ID', 'unidentifiableid', 'Combined_Sentiment', 'Combined_Wait',\n",
        "       'Updated_Wait',\n",
        "       'BERT_Sentiment', 'BERT_Wait', 'BERT_MistakeMedical',\n",
        "       'BERT_MistakeClerical', 'BERT_MistakeMedicalClerical',\n",
        "       'BERT_MistakeCommunication', 'BERT_MistakeAll', 'careprovidercomments',\n",
        "       'Medical_Mistakes', 'Clerical_Mistakes', 'Communication_Mistakes']\n",
        "# Drop columns not in the list\n",
        "full_data = full_data[columns_to_keep]\n",
        "\n",
        "# Save the combined data with predictions back to the original Excel file\n",
        "full_data.to_excel(file_path, index=False)  # Overwrite the original file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "Kfv3G61NWIBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "58acQU8lWIBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.disable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "FDkZFLBgWIBq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}